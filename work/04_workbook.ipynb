{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8edd1ed",
   "metadata": {},
   "source": [
    "# Exploring and Cleaning a Data Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d87c4",
   "metadata": {},
   "source": [
    " By: Dr. Eric Godat and Dr. Rob Kalescky\n",
    " \n",
    " Natural Language Toolkit: [Documentation](http://www.nltk.org/)\n",
    " \n",
    " Reference Text: [Natural Language Processing with Python](http://www.nltk.org/book/)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749dc1df",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ba050",
   "metadata": {},
   "source": [
    "These are the basic libraries we will use in for data manipulation (pandas) and math functions (numpy). We will add more libraries as we need them.\n",
    "\n",
    "As a best practice, it is a good idea to load all your libraries in a single cell at the top of a notebook, however for the purposes of this tutorial we will load them as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5e2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ad637",
   "metadata": {},
   "source": [
    "Load a data file into a pandas DataFrame.\n",
    "\n",
    "This tutorial was designed around using sets of data you have yourselves in a form like a CSV, TSV, or TXT file.  Feel free to use any set of data, but for now we will use a dataset created from scraping this [Multilingual Folktale Database](http://www.mftd.org/).\n",
    "\n",
    "This file is a CSV filetype, common for text data, but your data may also be stored as TSV's, TXT's, or other file types.  This will slightly change how you read from Pandas, but the concept is largely the same for the different filetypes.  Just keep this in mind when you see references to CSV.\n",
    "\n",
    "To proceed, you will need to have this file downloaded and in the same folder as this notebook. Alternatively you can put the full path to the file.  Typically, your program will look for the file with the name you specified in the folder that contains your program unless you give the program a path to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85dfc8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ATU Code</th>\n",
       "      <th>Author</th>\n",
       "      <th>Country of Origin</th>\n",
       "      <th>Original Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Story</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Title</th>\n",
       "      <th>Translated</th>\n",
       "      <th>Year Translated</th>\n",
       "      <th>Year Written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Jacob &amp; Wilhelm Grimm</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Katze und Maus in Gesellschaft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>Stealing the Partner's Butter (ATU 15)\\n\\t\\t</td>\n",
       "      <td>Cat and mouse in partnership</td>\n",
       "      <td>Margaret Hunt</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>123.0</td>\n",
       "      <td>Jacob &amp; Wilhelm Grimm</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Der Wolf und die sieben jungen Geisslein</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>The Wolf and the Seven Young Kids (ATU 123)\\n...</td>\n",
       "      <td>The Wolf and the Seven Young Kids</td>\n",
       "      <td>Margaret Hunt</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>516.0</td>\n",
       "      <td>Jacob &amp; Wilhelm Grimm</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Der treue Johannes</td>\n",
       "      <td>The Blue Fairy Book (nr. 30)</td>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>The Petrified Friend (ATU 516)\\n\\t\\t</td>\n",
       "      <td>Trusty John</td>\n",
       "      <td>Andrew Lang</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jacob &amp; Wilhelm Grimm</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Der gute Handel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The good bargain</td>\n",
       "      <td>Margaret Hunt</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>151.0</td>\n",
       "      <td>Jacob &amp; Wilhelm Grimm</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Der wunderliche Spielmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>Music lessons for wild animals (ATU 151)\\n\\t\\t</td>\n",
       "      <td>The Wonderful Musician</td>\n",
       "      <td>Translated into English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ATU Code                 Author Country of Origin  \\\n",
       "0          17      15.0  Jacob & Wilhelm Grimm           Germany   \n",
       "1          44     123.0  Jacob & Wilhelm Grimm           Germany   \n",
       "2          54     516.0  Jacob & Wilhelm Grimm           Germany   \n",
       "3          64       NaN  Jacob & Wilhelm Grimm           Germany   \n",
       "4          74     151.0  Jacob & Wilhelm Grimm           Germany   \n",
       "\n",
       "                             Original Title                        Source  \\\n",
       "0            Katze und Maus in Gesellschaft                           NaN   \n",
       "1  Der Wolf und die sieben jungen Geisslein                           NaN   \n",
       "2                        Der treue Johannes  The Blue Fairy Book (nr. 30)   \n",
       "3                           Der gute Handel                           NaN   \n",
       "4                 Der wunderliche Spielmann                           NaN   \n",
       "\n",
       "                                               Story  \\\n",
       "0  A cat had made the acquaintance of a mouse, an...   \n",
       "1  There was once an old goat who had seven littl...   \n",
       "2  ONCE upon a time there was an old king who was...   \n",
       "3  There was once a peasant who had driven his co...   \n",
       "4  There was once a wonderful musician, who went ...   \n",
       "\n",
       "                                          Story Type  \\\n",
       "0       Stealing the Partner's Butter (ATU 15)\\n\\t\\t   \n",
       "1   The Wolf and the Seven Young Kids (ATU 123)\\n...   \n",
       "2               The Petrified Friend (ATU 516)\\n\\t\\t   \n",
       "3                                                NaN   \n",
       "4     Music lessons for wild animals (ATU 151)\\n\\t\\t   \n",
       "\n",
       "                               Title               Translated  \\\n",
       "0       Cat and mouse in partnership            Margaret Hunt   \n",
       "1  The Wolf and the Seven Young Kids            Margaret Hunt   \n",
       "2                        Trusty John              Andrew Lang   \n",
       "3                   The good bargain            Margaret Hunt   \n",
       "4             The Wonderful Musician  Translated into English   \n",
       "\n",
       "   Year Translated Year Written  \n",
       "0           1884.0         1812  \n",
       "1           1884.0         1812  \n",
       "2           1889.0         1812  \n",
       "3           1884.0         1812  \n",
       "4              NaN         1812  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/folktales.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2107e4",
   "metadata": {},
   "source": [
    "Here we can see all the information available to us from the file in the form of a Pandas DataFrame. For the remainder of this tutorial, we will focus primarily on the full text of each data chunk, which we will name the *Story* column.  With your data set this is likely to be something very different, so feel free to call is something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a2822",
   "metadata": {},
   "source": [
    "## Counting Words and Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b7d87",
   "metadata": {},
   "source": [
    "The first bit of analysis we might want to do is to count the number of words in one piece of data. To do this we will add a column called *wordcount* and write an operation that applies a function to every row of the column.\n",
    "\n",
    "Unpacking this piece of code, *len(str(x).split(\" \")*, tells us what is happening.\n",
    "\n",
    "For the content of cell *x*, convert it to a string, *str()*, then split that string into pieces at each space, *split()*.\n",
    "\n",
    "The result of that is a list of all the words in the text and then we can count the length of that list, *len()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5a0656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  wordcount\n",
       "0  A cat had made the acquaintance of a mouse, an...        987\n",
       "1  There was once an old goat who had seven littl...       1037\n",
       "2  ONCE upon a time there was an old king who was...       3068\n",
       "3  There was once a peasant who had driven his co...       1723\n",
       "4  There was once a wonderful musician, who went ...       1066"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['wordcount'] = data['Story'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['Story','wordcount']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac51750",
   "metadata": {},
   "source": [
    "We can do something similar to count the number of characters in the data chunk, including spaces. If you wanted to exclude whitespaces, you could take the list we made above, join it together and count the length of the resulting string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d71d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"No Information Provided\") #If some of our data is missing, this will replace the blank entries. This is only necessary in some cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993c86ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>5108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>15999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>8907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>5592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  char_count\n",
       "0  A cat had made the acquaintance of a mouse, an...        5108\n",
       "1  There was once an old goat who had seven littl...        5343\n",
       "2  ONCE upon a time there was an old king who was...       15999\n",
       "3  There was once a peasant who had driven his co...        8907\n",
       "4  There was once a wonderful musician, who went ...        5592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['char_count'] = data['Story'].str.len() ## this also includes spaces, to do it without spaces, you could use something like this: \"\".join()\n",
    "data[['Story','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5e54",
   "metadata": {},
   "source": [
    "Now we want to calculate the average word length in the data.\n",
    "\n",
    "Let's define a function that will do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418d2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44a24c",
   "metadata": {},
   "source": [
    "We can now apply that function to all the data chunks and save that in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9141ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>4.176292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>4.153327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>4.215124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>4.170052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>4.246717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  avg_word\n",
       "0  A cat had made the acquaintance of a mouse, an...  4.176292\n",
       "1  There was once an old goat who had seven littl...  4.153327\n",
       "2  ONCE upon a time there was an old king who was...  4.215124\n",
       "3  There was once a peasant who had driven his co...  4.170052\n",
       "4  There was once a wonderful musician, who went ...  4.246717"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['avg_word'] = data['Story'].apply(lambda x: avg_word(x))\n",
    "data[['Story','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8120e4",
   "metadata": {},
   "source": [
    "We can then sort by the average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c35077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>A GNAT settled on the horn of a Bull, and sa...</td>\n",
       "      <td>3.623377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>A MAN wished to purchase an Ass, and agreed ...</td>\n",
       "      <td>3.709924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>A HOUND having started a Hare on the hillsid...</td>\n",
       "      <td>3.737374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Once on a time there was a little boy who was ...</td>\n",
       "      <td>3.771277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Hare once said to Elephant : \" Let us play hid...</td>\n",
       "      <td>3.795652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Story  avg_word\n",
       "659    A GNAT settled on the horn of a Bull, and sa...  3.623377\n",
       "656    A MAN wished to purchase an Ass, and agreed ...  3.709924\n",
       "620    A HOUND having started a Hare on the hillsid...  3.737374\n",
       "391  Once on a time there was a little boy who was ...  3.771277\n",
       "895  Hare once said to Elephant : \" Let us play hid...  3.795652"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Story','avg_word']].sort_values(by='avg_word', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d614e",
   "metadata": {},
   "source": [
    "### Count Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cae70",
   "metadata": {},
   "source": [
    "Stopwords are words that are commonly used and do little to aid in the understanding of the content of a text. There is no universal list of stopwords and they vary on the style, time period and media from which your text came from.  Typically, people choose to remove stopwords from their data, as it adds extra clutter while the words themselves provide little to no insight as to the nature of the data.  For now, we are simply going to count them to get an idea of how many there are.\n",
    "\n",
    "For this tutorial, we will use the standard list of stopwords provided by the Natural Language Toolkit python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f29becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/shelbym/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84a461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a20f91",
   "metadata": {},
   "source": [
    "To count the number of stopwords in a chunk of data, we make a list of all the words in our data that are also in the stopword list. We can then just take the length of that list and store it in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feecca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>446</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>505</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>1471</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>745</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>470</td>\n",
       "      <td>1066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  stopwords  wordcount\n",
       "0  A cat had made the acquaintance of a mouse, an...        446        987\n",
       "1  There was once an old goat who had seven littl...        505       1037\n",
       "2  ONCE upon a time there was an old king who was...       1471       3068\n",
       "3  There was once a peasant who had driven his co...        745       1723\n",
       "4  There was once a wonderful musician, who went ...        470       1066"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stopwords'] = data['Story'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data[['Story','stopwords','wordcount']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d683f",
   "metadata": {},
   "source": [
    "### Other Ways to Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f73566",
   "metadata": {},
   "source": [
    "There are other types of counting we might want to do. These might be more or less relevant depending on the test you are working with.\n",
    "\n",
    "For completeness, we have put them here but we will skip over them in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22a64d",
   "metadata": {},
   "source": [
    "#### Counting Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88518f3",
   "metadata": {},
   "source": [
    "This is really only useful for Twitter or other Internet texts but you could imagine wanting to count quotations or exclamations with something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1d014d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>special_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  special_char\n",
       "0  A cat had made the acquaintance of a mouse, an...             0\n",
       "1  There was once an old goat who had seven littl...             0\n",
       "2  ONCE upon a time there was an old king who was...             0\n",
       "3  There was once a peasant who had driven his co...             0\n",
       "4  There was once a wonderful musician, who went ...             0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['special_char'] = data['Story'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "data[['Story','special_char']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc638a50",
   "metadata": {},
   "source": [
    "#### Counting Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95945501",
   "metadata": {},
   "source": [
    "This counts the number of numerical digits in a text, which for strict text may not be helpful, but mostly numerical data will make more use of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef37c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Osborn’s PipeONCE on a time there was a poor t...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>ONCE FIVE women were reaping in a field. Each ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>There was once a husband and a wife who had tw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>There once lived an old man who had three daug...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Whoever, to his honor's cost,His pristine dign...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Story  numerics\n",
       "416  Osborn’s PipeONCE on a time there was a poor t...         9\n",
       "414  ONCE FIVE women were reaping in a field. Each ...         4\n",
       "696  There was once a husband and a wife who had tw...         2\n",
       "311  There once lived an old man who had three daug...         2\n",
       "727  Whoever, to his honor's cost,His pristine dign...         1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['numerics'] = data['Story'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data[['Story','numerics']].sort_values(by='numerics', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c56f038",
   "metadata": {},
   "source": [
    "#### Counting Uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8cd54",
   "metadata": {},
   "source": [
    "Counting uppercase words could give us an indication of how many sentences or proper nouns are in a text but this is likely too broad to be used to classify either on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8def37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cat had made the acquaintance of a mouse, an...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There was once an old goat who had seven littl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONCE upon a time there was an old king who was...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There was once a peasant who had driven his co...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There was once a wonderful musician, who went ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  upper\n",
       "0  A cat had made the acquaintance of a mouse, an...     12\n",
       "1  There was once an old goat who had seven littl...      5\n",
       "2  ONCE upon a time there was an old king who was...     39\n",
       "3  There was once a peasant who had driven his co...     35\n",
       "4  There was once a wonderful musician, who went ...     19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['upper'] = data['Story'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "data[['Story','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c7f6a",
   "metadata": {},
   "source": [
    "# Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126bc94",
   "metadata": {},
   "source": [
    "A major component of doing analysis on text is the cleaning of the text prior to the analysis.\n",
    "\n",
    "Though this process destroys some elements of the text (sentence structure, for example), it is often necessary in order to describe a text analytically. Depending on your choice of cleaning techniques, some elements might be preserved better than others if that is of importance to your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad66ef9",
   "metadata": {},
   "source": [
    "## Cleaning Up Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19ed57",
   "metadata": {},
   "source": [
    "This series of steps aims to clean up and standardize the text itself. This generally consists of removing common elements such as stopwords and punctuation but can be expanded to more detailed removals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884ab93",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685bd2",
   "metadata": {},
   "source": [
    "Here we enforce that all of the text is lowercase. This makes it easier to match cases and sort words.\n",
    "\n",
    "Notice we are assigning our modified column back to itself. This will save our modifications to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cbe1fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a cat had made the acquaintance of a mouse, an...\n",
       "1    there was once an old goat who had seven littl...\n",
       "2    once upon a time there was an old king who was...\n",
       "3    there was once a peasant who had driven his co...\n",
       "4    there was once a wonderful musician, who went ...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Story'] = data['Story'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee0abe",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e21d5",
   "metadata": {},
   "source": [
    "Here we remove all punctuation from the data. This allows us to focus on the words only as well as assist in matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6d8adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2953/3005510997.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Story'] = data['Story'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    a cat had made the acquaintance of a mouse and...\n",
       "1    there was once an old goat who had seven littl...\n",
       "2    once upon a time there was an old king who was...\n",
       "3    there was once a peasant who had driven his co...\n",
       "4    there was once a wonderful musician who went q...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Story'] = data['Story'].str.replace('[^\\w\\s]','')\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4483b0a",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd321b",
   "metadata": {},
   "source": [
    "Similar to what we did earlier when we counted stopwords, we now want to remove the stopwords. We will again use the NLTK list of stopwords but this time keep a list of words that do not appear in the list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a80e6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb106d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat made acquaintance mouse said much great lo...\n",
       "1    old goat seven little ones fond ever mother ch...\n",
       "2    upon time old king ill thought likely deathbed...\n",
       "3    peasant driven cow fair sold seven thalers way...\n",
       "4    wonderful musician went quite alone forest tho...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Story'] = data['Story'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664f7ec",
   "metadata": {},
   "source": [
    "### Remove Frequent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615c94a",
   "metadata": {},
   "source": [
    "If we want to catch common words that might have slipped through the stopword removal, we can build out a list of the most common words remaining in our text.\n",
    "\n",
    "Here we have built a list of the 10 most common words. Some of these words might actually be relevant to our analysis so it is important to be careful with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "501dc944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said      9644\n",
       "one       4980\n",
       "came      3502\n",
       "little    3489\n",
       "went      3317\n",
       "could     3040\n",
       "would     3023\n",
       "king      2846\n",
       "old       2764\n",
       "man       2375\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['Story']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff3f48",
   "metadata": {},
   "source": [
    "We now follow the same procedure with which we removed stopwords to remove the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef7abc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat made acquaintance mouse much great love fr...\n",
       "1    goat seven ones fond ever mother children day ...\n",
       "2    upon time ill thought likely deathbed send tru...\n",
       "3    peasant driven cow fair sold seven thalers way...\n",
       "4    wonderful musician quite alone forest thought ...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['Story'] = data['Story'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfca3c",
   "metadata": {},
   "source": [
    "### Remove Rare Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c873f16",
   "metadata": {},
   "source": [
    "By analogy, we can remove the most rare words. Some of these are strange misspellings or [hapax legomenon](https://en.wikipedia.org/wiki/Hapax_legomenon) (one off words that don't appear anywhere else in the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c68eff65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schoolmasterif    1\n",
       "understoodand     1\n",
       "kirstenbut        1\n",
       "goodthat          1\n",
       "readlater         1\n",
       "bookbinder        1\n",
       "themhans          1\n",
       "cagei             1\n",
       "hansthe           1\n",
       "moonfrom          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['Story']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca4b6e",
   "metadata": {},
   "source": [
    "Again, removing words following the same process as the stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8af4385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat made acquaintance mouse much great love fr...\n",
       "1    goat seven ones fond ever mother children day ...\n",
       "2    upon time ill thought likely deathbed send tru...\n",
       "3    peasant driven cow fair sold seven thalers way...\n",
       "4    wonderful musician quite alone forest thought ...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['Story'] = data['Story'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517ff5e",
   "metadata": {},
   "source": [
    "### Correct Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f107487",
   "metadata": {},
   "source": [
    "Misspellings can cause inaccuracies in text analysis. For example, \"the\" and \"teh\" are likely intended to be the same word and a reader might gloss over that typo, but a computer would view them as distinct.\n",
    "\n",
    "To help address this we will leverage the [TextBlob package](https://textblob.readthedocs.io/en/dev/) to check the spelling.\n",
    "\n",
    "Since this can be slow and often of questionable usefulness, we have limited the check to the first 5 rows of our DataFrame.\n",
    "\n",
    "It is also worth keeping in mind that, much like autocorrect on your phone, the spellchecking here is not going to be perfectly accurate and could result in just as many errors as it fixes (especially if you are working on text from edited or published sources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b419c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat made acquaintance mouse much great love fr...\n",
       "1    goat seven ones fond ever mother children day ...\n",
       "2    upon time ill thought likely deathbed send tru...\n",
       "3    peasant driven cow fair sold seven thaler way ...\n",
       "4    wonderful musician quite alone forest thought ...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "data['Story'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4f051",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e442f6",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting up a block of text into a sequence of words or sentences.\n",
    "\n",
    "For those familiar with R and the Tidyverse, this would be referred to as unnesting tokens\n",
    "\n",
    "Here we show all the tokenized words from the first data chunk in our Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0455c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83f8b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/users/shelbym/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/share/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/tokenizers.py:57\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m'''Return a list of sentences.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/users/shelbym/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/share/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m----> 3\u001b[0m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m---> 24\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/blob.py:649\u001b[0m, in \u001b[0;36mTextBlob.words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    If you want to include punctuation characters, access the ``tokens``\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    property.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    :returns: A :class:`WordList <WordList>` of word tokens.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WordList(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_punc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/tokenizers.py:73\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, include_punc, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, include_punc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124;03m\"\"\"Convenience function for tokenizing text into words.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    tokenized to sentences before being tokenized to words.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     words \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m     71\u001b[0m         _word_tokenizer\u001b[38;5;241m.\u001b[39mitokenize(sentence, include_punc\u001b[38;5;241m=\u001b[39minclude_punc,\n\u001b[1;32m     72\u001b[0m                                 \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/base.py:64\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[0;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    :rtype: generator\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "TextBlob(data['Story'][0]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7111536",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671551da",
   "metadata": {},
   "source": [
    "Stemming is the process of removing suffices, like \"ed\" or \"ing\".\n",
    "\n",
    "We will use another standard NLTK package, PorterStemmer, to do the stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a182054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat made acquaint mous much great love friends...\n",
       "1    goat seven one fond ever mother children day g...\n",
       "2    upon time ill thought like deathb send trusti ...\n",
       "3    peasant driven cow fair sold seven thaler way ...\n",
       "4    wonder musician quit alon forest thought manne...\n",
       "Name: Story, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data['Story'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7261bc",
   "metadata": {},
   "source": [
    "As we can see \"wonderful\" became \"wonder\", which could help an analysis, but \"deathbed\" became \"deathb\" which is less helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c248a",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8dc1f",
   "metadata": {},
   "source": [
    "Lemmatization is often a more useful approach than stemming because it leverages an understanding of the word itself to convert the word back to its root word. However, this means lemmatization is less aggressive than stemming (probably a good thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed03ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fadbc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/users/shelbym/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/share/nltk_data'\n",
      "    - '/users/shelbym/.conda/envs/ds_1300/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/users/shelbym/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/share/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/blob.py:146\u001b[0m, in \u001b[0;36mWord.lemmatize\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43m_wordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNOUN\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m _wordnet\u001b[38;5;241m.\u001b[39m_FILEMAP\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/users/shelbym/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/share/nltk_data'\n    - '/users/shelbym/.conda/envs/ds_1300/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([Word(word)\u001b[38;5;241m.\u001b[39mlemmatize() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()]))\n\u001b[1;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word\n\u001b[0;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()]))\n\u001b[1;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/ds_1300/lib/python3.9/site-packages/textblob/decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "data['Story'] = data['Story'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['Story'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a9904",
   "metadata": {},
   "source": [
    "## Visualizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bb8b7",
   "metadata": {},
   "source": [
    "Now, we are going to make a word cloud based on our data set. We're going to use the [wordcloud](https://amueller.github.io/word_cloud/) package to help us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc724093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "stop_words = set(stop)\n",
    "#stop_words.update([\"saw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6014cd",
   "metadata": {},
   "source": [
    "If you want to update the stopwords after you see the wordcloud, type them into the empty list above and remove the # sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \" \".join(data['Story'])\n",
    "wordcloud = WordCloud(stopwords=stop, background_color=\"black\").generate(word)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693575f",
   "metadata": {},
   "source": [
    "The above code creates a wordcloud based off all the words (except for stop words) in all of the stories. While this can be fun, it may not be as interesting as a wordcloud for a single story. Let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    print('Title: {}'.format(data['Title'][i]))\n",
    "    word = data['Story'][i]\n",
    "    wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate(word)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b140bb",
   "metadata": {},
   "source": [
    "# Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6dae88",
   "metadata": {},
   "source": [
    "At this point we have a several options for cleaning and structuring our text data. We learned how to load data, do basic cleaning and start to analyze our data with some simple counting methods."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
